
# Train doc2vec model (with or without pre-trained word embeddings)

import gensim.models as g
import logging

# doc2vec parameters
vector_size = 300                       # determines the number of vectors to be created
window_size = 15                        # maximum distance between the current and predicted word within a sentence
min_count = 1                           # Ignores all words with total frequency lower than this
sampling_threshold = 1e-5               # The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5)
negative_size = 5                       # if 0 no noise word can be drawn, but if set between (5-20) then that number of noise words can be drawn
train_epoch = 100                       # Number of iterations (epochs) over the corpus.
dm = 0 #0 = dbow; 1 = dmpv              # if set to 1 then distributed memory is used. and if set to 0 then distributed bag of words is used.
worker_count = 1                        # Use these many worker threads to train the model 

# pretrained word embeddings
pretrained_emb = "GoogleNewsvectors.bin"        # None if use without pretrained embeddings

# input corpus
train_corpus = "toy_data/train_docs_dataset.txt"

# output model
saved_path = "toy_data/model.bin"

# enable logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

# train doc2vec model
docs = g.doc2vec.TaggedLineDocument(train_corpus)
model = g.Doc2Vec(docs, size=vector_size, window=window_size, min_count=min_count, sample=sampling_threshold, workers=worker_count, hs=0, dm=dm, negative=negative_size, dbow_words=1, dm_concat=1, pretrained_emb=pretrained_emb, iter=train_epoch)

# save model
model.save(saved_path)
